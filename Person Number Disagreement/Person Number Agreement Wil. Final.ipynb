{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "086fda04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Alice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import copy\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from nltk.stem import WordNetLemmatizer as wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim.models\n",
    "import pickle\n",
    "import joblib\n",
    "import chardet\n",
    "import spacy\n",
    "from spacy import lookups\n",
    "from spacy import tokenizer\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18024182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data \n",
    "\n",
    "allData = pd.read_csv('person_num_revised_test.txt', delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "761f58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing and adds speaker column\n",
    "speakers = []\n",
    "for i in range(len(allData)):\n",
    "    if (':' in allData.iloc[i, 0]):\n",
    "        index = allData.iloc[i, 0].index(':')\n",
    "        speakers.append((allData.iloc[i, 0])[:index+1])\n",
    "        allData.iloc[i, 0] = re.sub(\"[a-zA-Z]:\\s+\", \"\", allData.iloc[i, 0]) \n",
    "    else:\n",
    "        speakers.append('F:')\n",
    "    allData.iloc[i, 0] = re.sub(\"\\s{2,}\", \" \", allData.iloc[i, 0])\n",
    "    allData.iloc[i, 0] = re.sub(\"’|‘\", \"'\", allData.iloc[i, 0]) #fixes apostrophes\n",
    "    allData.iloc[i, 0] = re.sub(\"—\", \"--\", allData.iloc[i, 0]) #m-dash was causing formatting issues, changed it to two dashes\n",
    "    allData.iloc[i, 0] = re.sub(\"“\", '\"', allData.iloc[i, 0]) #fixes quotation marks  \n",
    "allData.insert (0, \"Speaker\", speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d8028c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "# Creates a Sentence object\n",
    "# p is set to 0, but if the tagger finds instance of person number disagreement, will later be set to 1\n",
    "# text = text of Sentence\n",
    "# pna = which occurence of person number disagreement is being looked at in the sentence\n",
    "  # eg. if the variable be is set to 2, the model will observe whether the 2nd potential instance of disagreement\n",
    "    # is in fact disagreement\n",
    "# num = the index of the Sentence in the original csv\n",
    "    p = 0\n",
    "    r1 = 0; r2 = 0;\n",
    "    def __init__(self, text, pna, num):\n",
    "        self.text = text\n",
    "        self.pna = pna\n",
    "        self.num = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eae5782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproccesses test text for linguistic analysis\n",
    "\n",
    "lines = []\n",
    "\n",
    "with open(\"person_num_revised_test.txt\", \"r\", encoding=\"utf-8\") as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "    header = next(tsv_reader)\n",
    "    for row in tsv_reader:\n",
    "        row = row[:1]\n",
    "        #row = row[1:2]\n",
    "        row[0] = re.sub(\"[a-zA-Z]:\\s+\", \"\", row[0]) #removes the interviewer tag\n",
    "        row[0] = re.sub(\"\\s{2,}\", \" \", row[0]) #removes excessive spaces\n",
    "        row[0] = re.sub(\"’|‘\", \"'\", row[0]) #fixes apostrophes\n",
    "        row[0] = re.sub(\"—\", \"--\", row[0]) #m-dash was causing formatting issues, changed it to two dashes\n",
    "        row[0] = re.sub(\"“\", '\"', row[0]) #fixes quotation marks    \n",
    "        lines.append(row[0])\n",
    "\n",
    "sentences=lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "174702c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of Sentence objects with at least one potential instance of person number disagreement\n",
    "\n",
    "sen = []\n",
    "for line in range(len(lines)):\n",
    "        pna = 1\n",
    "        numPna = 0\n",
    "        parsed = nlp(lines[line])\n",
    "        for i, word in enumerate(parsed):\n",
    "            wordCounted = False \n",
    "            if(i+1 < len(parsed)):\n",
    "                j = 1\n",
    "                while (j < 4 and i+j < len(parsed)):\n",
    "                    after = parsed[i+j]\n",
    "                    if (wordCounted == False and (word.pos_ == \"NOUN\" or word.pos_ == \"PRON\" or word.pos_ == \"PROPN\" or word.pos_ == \"NUM\" or word.text.lower() == 'there') and (after.pos_ == \"AUX\" or after.pos_ == \"VERB\")):\n",
    "                        numPna += 1\n",
    "                        wordCounted = True\n",
    "                    j += 1\n",
    "        if (numPna >= 1):\n",
    "            sen.append(Sentence(lines[line], pna, line))\n",
    "        while (numPna > 1):\n",
    "            pna += 1\n",
    "            sen.append(Sentence(lines[line], pna, line))\n",
    "            numPna -= 1\n",
    "for Sen in range(len(sen)):\n",
    "    sen[Sen].text = (sen[Sen].text).replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5130384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets list of part of speech groups\n",
    "\n",
    "other_singular_pronouns= ['i', 'you']\n",
    "personal_object_pronouns = ['you', 'him', 'her', 'us', 'there']\n",
    "personal_object_pronouns_plural = ['we', 'you', 'us', 'there']\n",
    "thirdparty_singular_pronouns=['he','she','it']\n",
    "thirdparty_plural_pronouns=['we','they','you']\n",
    "aux_verbs1=['do','does']\n",
    "aux_verbs2=['was','were','has','have']\n",
    "singulardeterminants=['that','this'] \n",
    "pluraldeterminants=['these','those']\n",
    "pluralquantifiers = ['none', 'few', 'fewer', 'less', 'half', 'many', 'more', 'most', 'some', 'any', 'all', 'every']\n",
    "questionwords = ['do', 'did', 'why', 'what', 'where', 'who', 'why', 'how', 'when', 'whose'] # add in 'do'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f071577-7ffb-49fe-9b2a-426bf955f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags the Sentence objects with each rule\n",
    "\n",
    "index = -1\n",
    "senNum = 1\n",
    "numPna = 0\n",
    "pnaIndices = []\n",
    "\n",
    "\n",
    "modified_sentence=[]\n",
    "for Sentence1 in sen:\n",
    "        Sentence1.p = 0\n",
    "        nextword = None\n",
    "        index += 1 \n",
    "        numPna = 0\n",
    "        if (index > 0 and (sen[index].text == sen[index-1].text) and (sen[index].num == sen[index-1].num)):\n",
    "            senNum += 1\n",
    "        else:\n",
    "            senNum = 1\n",
    "        parsed = nlp(Sentence1.text)\n",
    "        options = {\"collapse_punct\": False}\n",
    "        tok_l = parsed.to_json()['tokens']\n",
    "        for t in tok_l:\n",
    "            head = tok_l[t['head']]\n",
    "        for i, word in enumerate(parsed):\n",
    "                head = tok_l[tok_l[i]['head']]\n",
    "                nextword = None\n",
    "                nextnextword = None\n",
    "                prevword = None\n",
    "                prevprevword = None\n",
    "                found = False\n",
    "                noDisagreement = False\n",
    "                count = 1\n",
    "# Identifies a potential subject\n",
    "                if i+1 in range(len(parsed)) and (word.pos_ == 'NOUN' or word.pos_ == 'PRON' or word.pos_ == 'PROPN' or word.pos_ == \"NUM\" or word.text.lower() == 'there'):\n",
    "                    head = tok_l[tok_l[i+count-1]['head']]\n",
    "                    nextword = parsed[i+count]\n",
    "                    if i+count > 0:\n",
    "                        prevword = parsed[i-1]\n",
    "                    if i+count > 1:\n",
    "                        prevprevword = parsed[i-2]\n",
    "                    if i+count > 2:\n",
    "                        prevprevprevword = parsed[i-3]\n",
    "                    if i+count+2 in range(len(parsed)):\n",
    "                        nextnextword = parsed[i+count+1]\n",
    "                    while (found == False and nextword): # and (count < 10)):\n",
    "                        noDisagreement = False\n",
    "# Identifies a potential verb to the subject and checks that the the subect/verb combination a) have an appropriate dependency and b) have that dependency between each other or with a shared word\n",
    "                        if (nextword and (nextword.pos_ == \"VERB\" or nextword.pos_ == \"AUX\") and (parsed[i+count-1].text != 'to') and (((word.dep_ == \"nsubj\" or word.dep_ == \"nsubjpass\") and ((Sentence1.text[tok_l[i]['start']:tok_l[i]['end']] == word.text) and ((Sentence1.text[head['start']:head['end']] == nextword.text) or ((nextword.dep_ == 'aux' or nextword.dep_ == 'auxpass') and nextnextword and ((Sentence1.text[tok_l[i+count]['start']:tok_l[i+count]['end']] == nextword.text) and ((Sentence1.text[tok_l[tok_l[i+count]['head']]['start']:tok_l[tok_l[i+count]['head']]['end']] == Sentence1.text[head['start']:head['end']]))))))) or ((word.dep_ == \"conj\" or (nextword.dep_ == \"relcl\" and nextword.pos_ == \"AUX\") or word.dep_ == \"csubj\" or (word.dep_ == 'pobj' and nextword.pos_ == \"AUX\" and Sentence1.text[head['start']:head['end']] not in ['of', 'from']) or (word.dep_ == 'expl' and word.text.lower() == 'there'))))):    \n",
    "                            find = 0\n",
    "                            found = True\n",
    "                            numPna += 1\n",
    "                            if (numPna == senNum):\n",
    "                                # if a word has a particular part of speech and nsubj dependency and is preceded by a noun or pronoun, that noun or pronoun tends to indicate the subject tense in the phrase and should be designated as the subject against which the rules are performed\n",
    "                                if word.tag_ in ['DT', 'WDT', 'WP'] and word.dep_ in ['nsubj', 'nsubjpass'] and prevword.pos_ in ['NOUN', 'PRON', 'PROPN', 'NUM']:\n",
    "                                    word = prevword\n",
    "                                # rule 1 -> subject is not plural and is third party singular pronoun followed by non-third party singular verb\n",
    "                                if (((word.text.lower() in thirdparty_singular_pronouns) and nextword.tag_ =='VBP')):\n",
    "                                    Sentence1.p = 1\n",
    "                                # rule 2 -> subject is singular and followed by a base or non-third party verb which is not 'was' and does not have a 'relcl' (relative clause) dependency\n",
    "                                if ((word.tag_ == 'NN' or word.tag_ == 'NNP') and (word.text.lower() not in pluralquantifiers) and (nextword.tag_ == 'VB' or nextword.tag_ == 'VBP') and nextword.text.lower() != 'was' and nextword.dep_ != 'relcl'):\n",
    "                                    Sentence1.p = 1\n",
    "                                # rule 3 -> if either of rules 1-2 are true (e.g. singular noun followed by plural verb), but the pronoun/verb combination preceded by a 'question' word, e.g. 'did', then there is no disagreement. Otherwise, if those rules are not true but the noun is singular (e.g. the verb is singular as the plural condition is not met) and preceded by a question verb, there is disagreement\n",
    "                                changed = 0\n",
    "                                if (nextword.text not in ['was', 'were'] and nextword.tag_ not in ['VBG', 'VBD'] and ((prevword and prevword.text.lower() in questionwords and Sentence1.text[tok_l[i+count-2]['start']:tok_l[i+count-2]['end']] == prevword.text and Sentence1.text[tok_l[tok_l[i+count-2]['head']]['start']:tok_l[tok_l[i+count-2]['head']]['end']] == nextword.text) or (prevprevword and prevprevword.text.lower() in questionwords and Sentence1.text[tok_l[i+count-3]['start']:tok_l[i+count-3]['end']] == prevprevword.text and Sentence1.text[tok_l[tok_l[i+count-3]['head']]['start']:tok_l[tok_l[i+count-3]['head']]['end']] == nextword.text) or (prevprevprevword and prevprevprevword.text.lower() in questionwords and Sentence1.text[tok_l[i+count-4]['start']:tok_l[i+count-4]['end']] == prevprevprevword.text and Sentence1.text[tok_l[tok_l[i+count-4]['head']]['start']:tok_l[tok_l[i+count-4]['head']]['end']] == nextword.text))):\n",
    "                                    if Sentence1.p == 1:\n",
    "                                        Sentence1.p = 0\n",
    "                                        changed = 1\n",
    "                                    elif ((changed == 0) and (Sentence1.p == 0) and (word.text.lower() in thirdparty_singular_pronouns) or ((word.tag_ == 'NN') and (word.text.lower() not in pluralquantifiers))):\n",
    "                                        Sentence.p = 1\n",
    "                                # rule 4 -> singular pronoun subject 'I' or 'you' followed by plural verb\n",
    "                                if ((word.text.lower() in other_singular_pronouns) and nextword.text.endswith('s') and nextword.text.lower() != 'was') and not nextword.text.endswith('ss'):\n",
    "                                    Sentence1.p = 1\n",
    "                                # rule 5 -> subject is plural followed by 3rd person singular verb\n",
    "                                if (((word.tag_ == 'NNS' or word.tag_ == 'NNPS' or (word in other_singular_pronouns))) and nextword.tag_ =='VBZ'): \n",
    "                                    Sentence1.p = 1\n",
    "                                # rule 6 -> subject is plural or in second person followed by 'was'\n",
    "                                if ((((word.text.lower() in thirdparty_plural_pronouns) or (word.text.lower() in personal_object_pronouns_plural)) or (word.pos_ == 'NUM' and (word.text != \"1\" and word.text.lower() != \"one\")) or (word.text.lower() in pluralquantifiers) or((word.tag_ == 'NNS' or word.tag_ == 'NNPS'))) and (nextword.text.lower() == \"was\") and (parsed[i+1].tag_ != 'PRON' and parsed[i+1].tag_ != 'PRP')):\n",
    "                                    Sentence1.p = 1\n",
    "                                # rule 7 -> compound subject connected through 'and' where the verb is not plural\n",
    "                                if (parsed[i+1].text == \"and\" or word.dep_ == \"conj\") and (nextword.tag_ =='VBZ' or nextword.text == 'was'):\n",
    "                                    Sentence1.p = 1\n",
    "                                # rule 8 -> usage of there, e.g. determines whether 'were' or 'was' is appropriate given the context and checks whether that matches the subject tense\n",
    "                                if word.text.lower() == 'there' and (nextword.text.lower() == 'was' or nextword.text.lower() == 'were'):\n",
    "                                    nextnext = 1\n",
    "                                    while len(parsed) >= i+count+nextnext and (parsed[i+count+nextnext].pos_ == 'ADJ' or parsed[i+count+nextnext].pos_ == 'DET' or parsed[i+count+nextnext].pos_ == 'ADV' or parsed[i+count+nextnext].dep_ == 'compound'): \n",
    "                                        nextnext += 1\n",
    "                                    if (parsed[i+count+nextnext].pos_ == 'NUM' and parsed[i+count+nextnext].text != \"1\" and parsed[i+count+nextnext].text.lower() != \"one\") or (parsed[i+count+nextnext].text.lower() in pluralquantifiers) or ((parsed[i+count+nextnext].tag_ == 'NNS' or parsed[i+count+nextnext].tag_ == 'NNPS') and parsed[i+count+nextnext].pos_ != 'PROPN'):\n",
    "                                        if (nextword.text.lower() == \"was\"):\n",
    "                                            Sentence1.p = 1\n",
    "                                        else:\n",
    "                                            Sentence1.p = 0\n",
    "                                    elif (nextword.text.lower() == \"were\"):\n",
    "                                        Sentence1.p = 1\n",
    "                                    else:\n",
    "                                        Sentence1.p = 0\n",
    "                        count += 1\n",
    "                        if ((i+count) < len(parsed)) and (parsed[i].dep_ in ['nsubj', 'nsubjpass'] or parsed[i+count].dep_ not in ['nsubj', 'nsubjpass']):\n",
    "                            nextword = parsed[i+count]\n",
    "                        else:\n",
    "                            nextword = None\n",
    "                        if ((i+count+1) < len(parsed)):\n",
    "                            nextnextword = parsed[i+count+1]\n",
    "                        else:\n",
    "                            nextnextword = None\n",
    "                        if (nextword == None):\n",
    "                            found = True\n",
    "        if Sentence1.p != 1:\n",
    "            Sentence1.p = 0\n",
    "        modified_sentence.append(Sentence1)\n",
    "        \n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c132356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation pna values of duplicated sentences\n",
    "# e.g. if any have disagreement, assign all pna values to 1, else keep them as 0\n",
    "\n",
    "multPnainSentence = []\n",
    "\n",
    "for i in range(len(sen)-1):\n",
    "    if (i < len(sen)-1):\n",
    "        j = i+1 \n",
    "        if (sen[i].text != sen[j].text):\n",
    "            if (sen[i].p == 1):\n",
    "                multPnainSentence.append(True)\n",
    "            else:\n",
    "                multPnainSentence.append(False)\n",
    "        while (sen[i].text == sen[j].text):\n",
    "            if (sen[i].p == 1 or sen[j].p == 1):\n",
    "                multPnainSentence.append(True)\n",
    "            else:\n",
    "                multPnainSentence.append(False)\n",
    "            if (sen[j].p == 1):\n",
    "                sen[i].p == sen[j].p\n",
    "            if (sen[i].p == 1):\n",
    "                sen[j].p == sen[i].p\n",
    "            if (j < len(sen)-1):\n",
    "                j +=1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "if len(multPnainSentence) != len(sen):\n",
    "    if (sen[-1].p == 1):\n",
    "        multPnainSentence.append(True)\n",
    "    else:\n",
    "        multPnainSentence.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b63dec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes back to the original data and annotates each sentences with person number agreement or disagreement\n",
    "\n",
    "personNum = []\n",
    "for i in range(len(allData)):\n",
    "    m = 0\n",
    "    for j in range(len(sen)):\n",
    "        if i == sen[j].num:\n",
    "            if (m != 1):\n",
    "                m = sen[j].p\n",
    "            if (m == 1 and (\"%PN\" not in allData.iloc[i, allData.columns.get_loc('Speaker')])):\n",
    "                allData.loc[i, 'Speaker'] = allData.iloc[i, allData.columns.get_loc('Speaker')] + \" %PN\"\n",
    "    personNum.append(m)\n",
    "allData['personNumberAgreement'] = personNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a37734f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   agreement       0.99      0.92      0.96      3629\n",
      "disagreement       0.30      0.86      0.44       139\n",
      "\n",
      "    accuracy                           0.92      3768\n",
      "   macro avg       0.65      0.89      0.70      3768\n",
      "weighted avg       0.97      0.92      0.94      3768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "\n",
    "predictions = allData['personNumberAgreement']\n",
    "y = allData['Person/num. agreement']\n",
    "target_names = ['agreement', 'disagreement']\n",
    "print(classification_report(y, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89932b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write annotations to txt file\n",
    "\n",
    "with open('personNum.txt', 'w+', newline='') as file:\n",
    "   allData.to_csv('personNum.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fb8a32c-3f08-4479-95bb-7eace3f3d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write falsely predicted sentences to txt file\n",
    "\n",
    "falsePredicted = pd.DataFrame()\n",
    "for i in range(len(allData)):\n",
    "    if allData.loc[allData.index[i], 'Person/num. agreement'] != allData.loc[allData.index[i], 'personNumberAgreement']:\n",
    "        falsePredicted = pd.concat([falsePredicted, allData.iloc[[i]]], ignore_index=True)\n",
    "with open('falsePredictedPNA.txt', 'w+', newline='') as file:\n",
    "   falsePredicted.to_csv('falsePredictedPNA.txt', sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
